{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlntUONI29MU"
      },
      "source": [
        "# 6 Problemset \n",
        "\n",
        "**Instructions:** Fill in the missing code, which is marked by `# TODO`. Preferably run this notebook on a GPU. On Google Colab, you can do this by selecting Runtime > Change Runtime Type > Hardware Acceleration > GPU. Start by loading the necessary libraries as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wH8ZAy3W20dA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL9C8lehqWBM"
      },
      "source": [
        "## 6.4 Random overparameterized networks\n",
        "\n",
        "Exlain what the following code does:\n",
        "\n",
        "*   What does the plot show?\n",
        "*   How does the initialization of the network parameters depend on the number $N$ of neurons?\n",
        "*   What is the distribution of `model.predict([x])` for large $N$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "CNzb8Xy032yB",
        "outputId": "23703f46-9f48-4325-f6d8-0bad60a7fbe6"
      },
      "outputs": [],
      "source": [
        "N=1000\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(20):\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1)),\n",
        "    tf.keras.layers.Dense(N, activation='relu', bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.5), kernel_initializer=tf.keras.initializers.RandomNormal(stddev = 1.)),\n",
        "    tf.keras.layers.Dense(1, bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.5), kernel_initializer=tf.keras.initializers.RandomNormal(stddev = 1./np.sqrt(N)))\n",
        "  ])\n",
        "  ax.plot(np.linspace(-1, 1, 1000), model.predict(np.linspace(-1, 1, 1000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. The plot shows 20 trials on random prediction models on the input space [-1, 1]\n",
        "2. The first layer consists of N neurons, each neuron is formulated as $y_i=\\rho(a_ix+b_i)$. $\\forall i$, $a_i$ is intialized with standard deviation 1, and $b_i$ is intialized with standard deviation 0.5. The second layer is formulated as: $z = \\sum_{i=1}^{N}c_iy_i+d_i$. $\\forall i$, $c_i$ is intialized with standard deviation $1/\\sqrt{N}$, and $b_i$ is intialized with standard deviation 0.5. Hence, N controls the error bound, the larger N is, the more random error it generates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmcn_fRhFkD"
      },
      "source": [
        "## 6.5 Gradient flow of overparameterized networks\n",
        "\n",
        "Explain what the following code does:\n",
        "\n",
        "*   What does the plot show?\n",
        "*   The class `SaveWeights` saves the network weights during training. How does this work? \n",
        "*   Investigate the dependence on the number $N$ of hidden neurons: how many epochs do you need to get convergence to a (hopefully) nice interpolant of the data points?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XJ5uJlGIm_rd",
        "outputId": "8677943d-5e4c-4df3-e843-709a726f2c75"
      },
      "outputs": [],
      "source": [
        "N=1000\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1)),\n",
        "    tf.keras.layers.Dense(N, activation='relu', bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.5), kernel_initializer=tf.keras.initializers.RandomNormal(stddev = 1.)),\n",
        "    tf.keras.layers.Dense(1, bias_initializer=tf.keras.initializers.RandomNormal(stddev=0.5), kernel_initializer=tf.keras.initializers.RandomNormal(stddev = 1./np.sqrt(N)))\n",
        "  ])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "x_train = np.linspace(-1, 1, 10)\n",
        "y_train = np.sin(np.pi*x_train)\n",
        "data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(10)\n",
        "\n",
        "class SaveWeights(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, interval=1):\n",
        "    super(SaveWeights, self).__init__()\n",
        "    self.interval = interval\n",
        "    self.weights = []\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch % self.interval == 1:\n",
        "      self.weights.append(self.model.get_weights())\n",
        "\n",
        "callback = SaveWeights(interval=100)\n",
        "\n",
        "history = model.fit(data_train, epochs=1000, verbose=0, callbacks=[callback])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_train, y_train, '.g')\n",
        "for weights in callback.weights:\n",
        "  model.set_weights(weights)\n",
        "  ax.plot(np.linspace(-1, 1, 1000), model.predict(np.linspace(-1, 1, 1000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_FO9h4wMJ2P"
      },
      "source": [
        "## 6.6 Expected risk of overparameterized networks\n",
        "\n",
        "For the learning task in 6.5, \n",
        "\n",
        "*   Create a test dataset\n",
        "*   Plot the training loss and test loss of the trained network as a function over the number $N$ of hidden neurons.\n",
        "*   What's a good choice of $N$? What does the theory say, and what do you see in practice?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
